{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install vosk\n",
        "!pip install jiwer\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Assuming the vosk model is not yet installed\n",
        "!wget https://alphacephei.com/vosk/models/vosk-model-en-us-0.22-lgraph.zip\n",
        "!unzip vosk-model-en-us-0.22-lgraph.zip -d model\n"
      ],
      "metadata": {
        "id": "sZbaTpkSYzjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/drive/MyDrive/Projects/AudioFiles_final/voices_final_trimmed_2.zip\" -d \"/content/\"\n"
      ],
      "metadata": {
        "id": "Vf_nEYcEZErh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correct paths assuming the structure mentioned and process them correctly\n",
        "audio_paths_file = '/content/audio_paths_file.txt'\n",
        "text_file = '/content/text_file.txt'\n",
        "\n",
        "# Load the paths and texts\n",
        "with open(audio_paths_file, 'r') as f:\n",
        "    # Assuming each line is \"ID PATH\" and you need the PATH part\n",
        "    audio_paths = [line.strip().split(' ')[1] for line in f.readlines()]\n",
        "\n",
        "with open(text_file, 'r') as f:\n",
        "    transcriptions = [line.strip() for line in f.readlines()]\n",
        "\n",
        "assert len(audio_paths) == len(transcriptions), \"Mismatch between audio paths and transcriptions count\"\n"
      ],
      "metadata": {
        "id": "kkXM3arpZ276"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from vosk import Model, KaldiRecognizer\n",
        "import wave\n",
        "import json\n",
        "import pandas as pd\n",
        "from jiwer import wer, cer\n",
        "import numpy as np\n",
        "\n",
        "model_path = './model/vosk-model-en-us-0.22-lgraph'\n",
        "model = Model(model_path)\n"
      ],
      "metadata": {
        "id": "7N6Xh7-FZ_eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe_audio(file_path, model):\n",
        "    try:\n",
        "        wf = wave.open(file_path, \"rb\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return \"\"\n",
        "    rec = KaldiRecognizer(model, wf.getframerate())\n",
        "    rec.SetWords(True)\n",
        "\n",
        "    while True:\n",
        "        data = wf.readframes(4000)\n",
        "        if len(data) == 0:\n",
        "            break\n",
        "        rec.AcceptWaveform(data)\n",
        "    wf.close()\n",
        "    return json.loads(rec.FinalResult())['text']\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_transcriptions(audio_paths, transcriptions, model):\n",
        "    evaluation_results = []\n",
        "    for audio_path, transcription in tqdm(zip(audio_paths, transcriptions), total=len(audio_paths), desc=\"Evaluating Transcriptions\"):\n",
        "        try:\n",
        "            predicted_transcription = transcribe_audio(audio_path, model)\n",
        "            evaluation_results.append({\n",
        "                'audio_path': audio_path,\n",
        "                'predicted_transcription': predicted_transcription,\n",
        "                'ground_truth': transcription,\n",
        "                'wer': wer(transcription, predicted_transcription),\n",
        "                'cer': cer(transcription, predicted_transcription)\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {audio_path}: {e}\")\n",
        "    return pd.DataFrame(evaluation_results)\n",
        "\n"
      ],
      "metadata": {
        "id": "SH3LOau_aDu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = evaluate_transcriptions(audio_paths, transcriptions, model)\n"
      ],
      "metadata": {
        "id": "Lk5flbaQaHbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_wer = np.mean(results_df['wer']) * 100\n",
        "overall_cer = np.mean(results_df['cer']) * 100\n",
        "\n",
        "results_text = f\"\"\"Dataset: /content/output_data_directory/eval_dataset\n",
        "WER: {overall_wer:.2f}%, CER: {overall_cer:.2f}%\\n\"\"\"\n",
        "\n",
        "for index, row in results_df.iterrows():\n",
        "    results_text += f'\\nReference: \"{row[\"ground_truth\"]}\"\\nPredicted:  \"{row[\"predicted_transcription\"]}\"\\n'\n",
        "\n",
        "results_file_path = '/content/evaluation_results.txt'\n",
        "with open(results_file_path, 'w') as file:\n",
        "    file.write(results_text)\n",
        "\n",
        "print(f'Results saved to {results_file_path}')\n"
      ],
      "metadata": {
        "id": "7untMtcialL6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}